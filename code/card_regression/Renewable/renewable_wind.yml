data:
    dataset: "renewable"
    type: "wind"
    dir: "data"
    window_size: 5
    reduce_1d: True
    add_prev: True
    n_splits: 1
    num_workers: 2
    normalize_x: True
    normalize_y: True
    data_root: "../../"

model:
    type: "simple"
    x_dim: 285
    y_dim: 1
    cat_x: True
    feature_dim: 256
    var_type: fixedlarge
    ema_rate: 0.9999
    ema: True

diffusion:
    beta_schedule: linear  # cosine
    beta_start: 0.0001
    beta_end: 0.02
    timesteps: 1000
    vis_step: 100
    num_figs: 10
    conditioning_signal: "NN"
    nonlinear_guidance:
        pre_train: True
        joint_train: True
        n_pretrain_epochs: 500
        logging_interval: 10
        hid_layers: [256, 128, 64]
        use_batchnorm: False
        negative_slope: 0.01
        dropout_rate: 0.1
        apply_early_stopping: True
        n_pretrain_max_epochs: 1000
        patience: 50
        delta: 0  # hyperparameter for improvement measurement in the early stopping scheme

training:
    batch_size: 256
    n_epochs: 500
    snapshot_freq: 1000000000
    logging_freq: 5000
    validation_freq: 10000
    image_folder: 'training_image_samples'

sampling:
    batch_size: 256
    sampling_size: 100
    last_only: True
    image_folder: 'sampling_image_samples'

testing:
    batch_size: 256
    sampling_size: 100
    last_only: True
    plot_freq: 5
    image_folder: 'testing_image_samples'
    n_z_samples: 100
    n_bins: 10
    compute_metric_all_steps: True
    mean_t: 0
    coverage_t: 0
    nll_t: 0
    trimmed_mean_range: [0.0, 100.0]
    PICP_range: [2.5, 97.5]
    make_plot: False
    squared_plot: False
    plot_true: False
    plot_gen: False
    fig_size: [8, 5]

optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0

aux_optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0